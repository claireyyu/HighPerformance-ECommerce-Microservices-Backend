package queues

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"strings"
	"time"

	"github.com/IBM/sarama"
)

type KafkaProducer struct {
	producer sarama.SyncProducer
}

func NewKafkaProducer(brokers string) (*KafkaProducer, error) {
	config := sarama.NewConfig()
	config.Producer.RequiredAcks = sarama.WaitForAll
	config.Producer.Retry.Max = 5
	config.Producer.Idempotent = true

	// åˆ†åŒºç­–ç•¥é…ç½®
	config.Producer.Partitioner = sarama.NewHashPartitioner
	config.Producer.Return.Successes = true

	// ğŸš€ æ‰¹å¤„ç†é…ç½®
	config.Producer.Flush.Messages = 500
	config.Producer.Flush.Frequency = 50 * time.Millisecond
	config.Producer.Flush.MaxMessages = 500

	config.Producer.Compression = sarama.CompressionSnappy

	producer, err := sarama.NewSyncProducer(strings.Split(brokers, ","), config)
	if err != nil {
		return nil, fmt.Errorf("failed to create Kafka producer: %v", err)
	}

	return &KafkaProducer{producer}, nil
}

func (kp *KafkaProducer) PublishToKafka(topic, message string) error {
	// ä½¿ç”¨ç”¨æˆ·IDä½œä¸ºåˆ†åŒºkeyï¼Œç¡®ä¿åŒä¸€ç”¨æˆ·çš„è®¢å•è¿›å…¥åŒä¸€åˆ†åŒº
	var key string
	var order struct {
		UserID int `json:"user_id"`
	}
	if err := json.Unmarshal([]byte(message), &order); err == nil {
		key = fmt.Sprintf("%d", order.UserID)
	}

	msg := &sarama.ProducerMessage{
		Topic: topic,
		Key:   sarama.StringEncoder(key),
		Value: sarama.StringEncoder(message),
	}
	_, _, err := kp.producer.SendMessage(msg)
	return err
}

func (kp *KafkaProducer) Close() error {
	return kp.producer.Close()
}

// ----- âœ… Kafka Consumer Group with Goroutine Pool -----

type kafkaHandler struct {
	handler func(string) error
	pool    chan struct{}
}

func (h *kafkaHandler) Setup(sarama.ConsumerGroupSession) error   { return nil }
func (h *kafkaHandler) Cleanup(sarama.ConsumerGroupSession) error { return nil }

func (h *kafkaHandler) ConsumeClaim(session sarama.ConsumerGroupSession, claim sarama.ConsumerGroupClaim) error {
	for msg := range claim.Messages() {
		h.pool <- struct{}{} // block if pool is full
		go func(m *sarama.ConsumerMessage) {
			defer func() { <-h.pool }()
			if err := h.handler(string(m.Value)); err != nil {
				log.Printf("âŒ Handler error: %v", err)
			}
			session.MarkMessage(m, "")
		}(msg)
	}
	return nil
}

// StartGroupConsumer creates a consumer group with limited concurrency
func StartGroupConsumer(brokers []string, topic string, handler func(string) error) {
	groupID := "ecommerce-group"

	config := sarama.NewConfig()
	config.Version = sarama.V2_1_0_0

	// æ¶ˆè´¹è€…ç»„å†å¹³è¡¡ç­–ç•¥ï¼šç²˜æ€§åˆ†åŒºåˆ†é…
	config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategySticky
	config.Consumer.Group.Rebalance.Timeout = 60 * time.Second
	config.Consumer.Offsets.Initial = sarama.OffsetNewest

	// ä¼˜åŒ–çš„Fetché…ç½®
	config.Consumer.Fetch.Min = 1
	config.Consumer.Fetch.Default = 1024 * 1024 // 1MB
	config.Consumer.MaxWaitTime = 100 * time.Millisecond

	consumerGroup, err := sarama.NewConsumerGroup(brokers, groupID, config)
	if err != nil {
		log.Fatalf("âŒ Failed to create Kafka consumer group: %v", err)
	}

	ctx := context.Background()
	h := &kafkaHandler{
		handler: handler,
		pool:    make(chan struct{}, 1000),
	}

	go func() {
		for {
			if err := consumerGroup.Consume(ctx, []string{topic}, h); err != nil {
				log.Printf("âŒ Error from consumer group: %v", err)
				time.Sleep(2 * time.Second)
			}
		}
	}()
}
